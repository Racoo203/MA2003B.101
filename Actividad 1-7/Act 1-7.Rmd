---
title: "Actividad 1.7 Análisis Factorial I"
author: "Raúl Correa Ocañas"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(psych)
library(polycor)
library(ggcorrplot)
library(performance)
library(GPArotation)
```

El problema del Lago

Se trata de varias variables limnológicas de varios lagos Neotropicales. Se hace un estudio para saber el grado de productividad potencial del lago (concentración de nutrientes y carbono orgánico disuelto) y la adecuación del hábitat en lo que se refiere a sus condiciones para la vida (profundidad, pH, conductividad, oxígeno disuelto y temperatura). Estos dos factores, productividad y hábitat, podrían explicar razonablemente la correlación observada entre las distintas variables. Se trata de hacer un análisis factorial en este contexto para comprobar si este modelo responde razonablemente a la realidad y es, por lo tanto, adecuado para explicar los siguientes datos.

# Importación de Datos.

```{r}
data_path = "../data/datoslago.csv"
df = read.csv(data_path)
head(df,5)
```

Realicen el análisis factorial, discutiendo y comentando los resultados obtendidos de: 

# Ejercicio 1.
Obtener la matriz de correlaciones y la matriz de valores p de significancia por pares.

```{r}
corr.test(df,adjust="none")
```
Según lo observado como resultado de salida para este comando, se tienen las correlaciones por combinación de variables y sus respectivos valores p. Estos valores indican la significancia estadística de los resultados obtenidos. Según lo observado en la segunda tabla, se acepta una hipotesis nula para la correlación de las variables Temperatura y LnProteinas, indicando que su correlación no es estadísticamente significativa. Se observa que las variables con mejor correlaciones son: LnTamaño y Tamaño, LnTamaño y Proteina, y LnProteina y Proteina.

# Ejercicio 2.
Hacer una gráfica de la matriz de correlaciones.

```{r}
mat_cor = hetcor(df)$correlations
ggcorrplot(mat_cor,type="lower",hc.order = T)
```

En esta gráfica podemos ver de manera visual la correlación de las variables, mientras más rojas con mayor correlacion positiva y mientras más azules más correlacion negativa. En la gráfica se excluyen la repetición de las combinaciones de las variables, por lo que permite un mejor analisis de que correlaciones tienen mayor intensidad. El gráfico sugiere que Ln Tamaño y Tamaño tienen una fuerte correlación positiva, y después hay otros tres pares de combinaciones que visualmente no se puede apreciar la diferencia de correlaciones entre estos, por lo que es más complicado observar cual tendría mayor o menor correlación.


# Ejercicio 3.
Aplicar una prueba de correlación conjunta a los datos para verficar si es aplicable el Análisis Factorial y concluir.

$H_0$: Las variables son ortogonales (matriz identidad)
$H_A$: Las variables no son ortogonales (difiere de matriz identidad)

```{r}
b = check_sphericity_bartlett(df)

b
b$chisq
b$p
b$dof
```
Según los resultados obtenidos en la prueba de esfericidad de Bartlet, se tiene un valor de p menor a 0.001, por lo que se puede inferir que existe suficiente evidencia estadística y correlación significante para poder realizar un análisis factorial. 


# Ejercicio 4.
Otra prueba para, para comprobar si el análisis factorial es viable, y muy citada, es la prueba KMO. Aplíquela a estos datos, ¿contradice los resutados del inciso anterior?

```{r}
cor_df = cor(df)

K = KMO(cor_df)
cat("El valor del estadístico es: ", K$MSA)
```

Según los resultados obtenidos con la prueba KMO, los resultados caerían en el intervalo 0.60 a 0.69, lo cual se considera mediocre según los autores. Aunque utilizar descripciones de este estilo no enriquecen mucho la información que se tiene de los datos, ciertamente contradice la certeza que se tiene en la prueba de Chi Cuadrada, la cual con mucha certeza (p < 0.001) indica que los datos son aptos para un análisis factorial. Sin embargo, un estadístico de 0.5 indica para terminos prácticos una aceptación de la correlación conjunta. Esto significa que se tiene argumentos para sustentar que ambas pruebas concuerdan en el resultado.

# Ejercicio 5.
Si los datos pasaron la prueba de los puntos anteriores 3 y 4, hacer un análisis factorial usando el criterio de máxima verosimilitud y el de mínimo residuo.

```{r}
# Análisis de máxima verosimilitud
fa_mle = fa(cor(df), nfactors = 2, rotate = "none", fm = "mle")

# Análisis de mínimo residuo
fa_minres = fa(cor(df), nfactors = 2, rotate = "none", fm = "minres")

# Comunalidades
communalities = data.frame(MLE = fa_mle$communality, MINRES = fa_minres$communality)
communalities = communalities[order(communalities$MLE, decreasing = TRUE), ]
communalities

eigenvalues = data.frame(MLE = fa_mle$values, MINRES = fa_minres$values)
eigenvalues = eigenvalues[order(eigenvalues$MLE, decreasing = TRUE), ]
eigenvalues


```
Se muestran las comunalidades de las variables, que indican qué proporción de la varianza de cada variable es explicada por los factores extraídos. En ambos modelos, LnTamaño, Tamaño y LnProteinas son explicadas a al menos el 90% de sus variabilidades con los factores propuestos. La variable Proteinas pueden ser explicadas a al menos un 77% con los factores propuestos en el método de maxima verosimilitud, y al menos un 60% en el método de mínimo residuo. La temperatura es altamente explicada utilizando minimos residuos, pero su explicación es pésima en el método de máxima verosimilitud. La variable Oxígeno tanto para ambos modelos no es adecuada, por lo que ambos métodos no explican con los dos factores esta variable.



# Ejercicio 6.
Determine el número de factores adecuado según el criterio del gráfico de Cattell

```{r}
scree(cor_df)
```
Tanto para el método de MLE como MINRES, se puede decidir escoger entre 2 y 3 variables para el análisis factorial. Seleccionar más de 3 variables implicaría poca explicación de los datos a costo de un aumento de dimensionalidad, por lo que en este caso se puede considerar aceptable explicar los datos con dos factores, teniendo como mejor opción el método de MINRES.

# Ejercicio 7.
Realicen los gráficos correspondientes a la rotación Varimax y quartimax de los datos e interpreten en equipo los resultados.

```{r}
rot = c("none", "varimax", "quartimax")
bi_mod = function(tipo){
biplot.psych(fa(df,nfactors = 2,fm="mle",rotate = tipo),main = "",col=c(2,3,4),pch = c(21,18),group = df[,"Tamano"])  }
sapply(rot,bi_mod)
```

```{r}
# Análisis de máxima verosimilitud
fa_mle_varimax = fa(cor(df), nfactors = 2, rotate = "varimax", fm = "mle")

# Análisis de mínimo residuo
fa_minres_varimax = fa(cor(df), nfactors = 2, rotate = "varimax", fm = "minres")

# Comunalidades
communalities_varimax = data.frame(MLE = fa_mle$communality, MINRES = fa_minres$communality)
communalities_varimax = communalities_varimax[order(communalities_varimax$MLE, decreasing = TRUE), ]
communalities

eigenvalues_varimax = data.frame(MLE = fa_mle_varimax$values, MINRES = fa_minres_varimax$values)
eigenvalues_varimax = eigenvalues_varimax[order(eigenvalues_varimax$MLE, decreasing = TRUE), ]
eigenvalues_varimax
```

```{r}
# Análisis de máxima verosimilitud
fa_mle_quartimax = fa(cor(df), nfactors = 2, rotate = "quartimax", fm = "mle")

# Análisis de mínimo residuo
fa_minres_quartimax = fa(cor(df), nfactors = 2, rotate = "quartimax", fm = "minres")

# Comunalidades
communalities_quartimax = data.frame(MLE = fa_mle$communality, MINRES = fa_minres$communality)
communalities_quartimax = communalities_quartimax[order(communalities_quartimax$MLE, decreasing = TRUE), ]
communalities

eigenvalues_quartimax = data.frame(MLE = fa_mle_quartimax$values, MINRES = fa_minres_quartimax$values)
eigenvalues_quartimax = eigenvalues_quartimax[order(eigenvalues_quartimax$MLE, decreasing = TRUE), ]
eigenvalues_quartimax
```



# Ejercicio 8.
¿Qué pueden concluir? ¿Resultó razonable para este caso el modelo de análisis factorial? Expliquen.
